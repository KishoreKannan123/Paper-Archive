# Research Paper Archive

## Contents 

* Computer Vision
    - [U-Net: Convolutional Networks for Biomedical Image Segmentation](./Papers/U-Net%20-%20Convolutional%20Networks%20for%20Biomedical%20Image%20Segmentation/)
    - [UNet++: A Nested U-Net Architecture for Medical Image Segmentation](./Papers/UNet%2B%2B%20-%20A%20Nested%20U-Net%20Architecture%20for%20Medical%20Image%20Segmentation/)
    - [PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation](./Papers/PointNet%20-%20Deep%20Learning%20on%20Point%20Sets%20for%203D%20Classification%20and%20Segmentation/)
    - [StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks](./Papers/StackGAN%20-%20Text%20to%20Photo-realistic%20Image%20Synthesis%20with%20Stacked%20Generative%20Adversarial%20Networks/)
    - [StyleGAN: A Style-Based Generator Architecture for Generative Adversarial Networks](./Papers/StyleGAN%20-%20A%20Style-Based%20Generator%20Architecture%20for%20Generative%20Adversarial%20Networks/)
    - [Learning to Paint With Model-based Deep Reinforcement Learning](./Papers/Learning%20to%20Paint%20With%20Model-based%20Deep%20Reinforcement%20Learning/)
    - [Image Inpainting - Generative Image Inpainting with Contextual Attention](./Papers/Image%20Inpainting%20-%20Generative%20Image%20Inpainting%20with%20Contextual%20Attention)
    - [Pix2Pix - Image-to-Image Translation with Conditional Adversarial Networks](./Papers/Pix2Pix%20-%20Image-to-Image%20Translation%20with%20Conditional%20Adversarial%20Networks/)
    - [YOLO - You Only Look Once: Unified, Real-Time Object Detection](./Papers/YOLO%20-%20You%20Only%20Look%20Once:%20Unified,%20Real-Time%20Object%20Detection)
    - [Semantic Segmentation - Pyramid Scene Parsing Network](./Papers/PSPNet%20-%20Pyramid%20Scene%20Parsing%20Network/)
    - [DINO: Emerging Properties in Self-Supervised Vision Transformers](Papers/DINO:%20-Emerging%20-Properties%20-in%20-Self-Supervised%20-Vision%20-Transformers)


* Natural Language Processing
    - [BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding](./Papers/BERT%20-%20Pre-training%20of%20Deep%20Bidirectional%20Transformers%20for%20Language%20Understanding)
    - [DrQA - Reading Wikipedia to Answer Open-Domain Questions](./Papers/DrQA%20-%20Reading%20Wikipedia%20to%20Answer%20Open-Domain%20Questions/)
    - [Knowing When To Look - Adaptive attention for image captioning](Papers/Knowing%20When%20to%20Look%20%20-%20Adaptive%20Attention%20for%20image%20captioning)


* Reinforcement Learning
    
    - [Asynchronous Methods for Deep Reinforcement Learning](./Papers/Asynchronous%20Methods%20for%20Deep%20Reinforcement%20Learning/)
    - [DQN - Playing Atari with Deep Reinforcement Learning](./Papers/Deep%20Q-Learning/)
    - [DRQN - Deep Recurrent Q-Learning for Partially Observable MDPs](./Papers/Deep%20Recurrent%20Q-Learning%20for%20Partially%20Observable%20MDPs/)
    - [PPO - Proximal Policy Optimization Algorithms](./Papers/Proximal%20Policy%20Optimization%20Algorithms/)

------

This will be a community-driven repository where people contribute by sharing their thoughts on different research papers they have come across as a simple readme file. This would benefit the contributors by acting as a documentation for future reference and once this archive becomes decently big it would benefit the larger community as a whole. There is no restriction regarding what papers you can add onto this archive - can be old, new anything. Hope this repository gets some good contributions. Happy reading :)

**To contribute** 
- Fork the repository
- Add the required files as described below.
- Make a pull request (everything will be merged)

### REPOSITORY Layout

    .
    ├── README.md               # Entry point to view all available papers in the archive. 
    ├── <PAPER-1> (a folder) `  # Every paper must be inside a folder titled as the paper name
        ├── assets              # Folder to contain images used in the README file to summarise the paper.
        └── README.md           # README file containing the summary itself.
    ├── <PAPER-2>
        ├── assets
        └── README.md
    ├── <PAPER-3>
        ├── assets
        └── README.md

### NOTE

- Please ensure that you create an entry point to the summary on the main README FILE (this one).
- Create seperate folders for each paper to add to the repository and keep a seperate folder inside each called assets for any images used in your summary. 
